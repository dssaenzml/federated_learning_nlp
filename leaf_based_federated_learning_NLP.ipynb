{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "leaf_based_federated_learning_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1pyjr-tDNl4Pj44nq3HKn3U4_Q0uNiZLI",
      "authorship_tag": "ABX9TyMVm0h6F0w3eHBB0/tky9gF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dssaenzml/federated_learning_nlp/blob/main/leaf_based_federated_learning_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUTLQoouWEOY"
      },
      "source": [
        "# Application of Federated Learning\n",
        "\n",
        "I am going to create an NLP solution using federated learning using three different simulated locations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkSgCcAnqULi"
      },
      "source": [
        "#### Clonning Github repo to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSH6BmPKoEWd",
        "outputId": "6e554ad9-f6a4-49a9-d05b-27aae4150212"
      },
      "source": [
        "! git clone https://github.com/dssaenzml/leaf.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'leaf'...\n",
            "remote: Enumerating objects: 752, done.\u001b[K\n",
            "remote: Total 752 (delta 0), reused 0 (delta 0), pack-reused 752\u001b[K\n",
            "Receiving objects: 100% (752/752), 6.78 MiB | 13.57 MiB/s, done.\n",
            "Resolving deltas: 100% (350/350), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r242JjeYp5G4"
      },
      "source": [
        "#### Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGGSx90vFA7K",
        "outputId": "867bb521-d3e9-4546-b2a3-692271633e13"
      },
      "source": [
        "import os\n",
        "os.chdir('leaf')\n",
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t\t\t\t\t LICENSE.md\t    README.md\n",
            "docs\t\t\t\t\t models\t\t    requirements.txt\n",
            "leaf_based_federated_learning_NLP.ipynb  paper_experiments\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ti4ip8gpZD3"
      },
      "source": [
        "%%capture\n",
        "\n",
        "! pip3 install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yeca1dTCqE0g"
      },
      "source": [
        "## Twitter Sentiment Analysis Experiments\n",
        "\n",
        "In this experiment, we reproduce the statistical analysis experiment conducted in the LEAF paper. Specifically, we investigate the effect of varying the minimum number of samples per user (for training) on model accuracy when training using `FedAvg` algorithm, using the LEAF framework.\n",
        "\n",
        "For this example, we shall use Sentiment140 dataset (containing 1.6 million tweets), and we shall train a 2-layer LSTM model with cross-entropy loss, and using pre-trained GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_poZzcyXqieR"
      },
      "source": [
        "### Experiment Setup and Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPzBUhbpsW-p"
      },
      "source": [
        "#### Pre-requisites\n",
        "\n",
        "Since this experiment requires pre-trained word embeddings, we recommend running the `models/sent140/get_embs.sh` file, which fetches 300-dimensional pretrained GloVe vectors.\n",
        "\n",
        "After extraction, this data is stored in `models/sent140/embs.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fEQeq19sYXf",
        "outputId": "a5c8b068-1006-4a24-c134-f667ebf9db2c"
      },
      "source": [
        "os.chdir('models/sent140')\n",
        "\n",
        "! sh ./get_embs.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./get_embs.sh: 3: cd: can't cd to sent140\n",
            "--2021-04-11 06:30:16--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-11 06:30:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-11 06:30:17--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 42s  \n",
            "\n",
            "2021-04-11 06:32:59 (5.08 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Killed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02-IommBu9rY"
      },
      "source": [
        "#### Dataset fetching and pre-processing\n",
        "\n",
        "LEAF contains powerful scripts for fetching and conversion of data into JSON format for easy utilization. Additionally, these scripts are also capable of subsampling from the dataset, and splitting the dataset into training and testing sets.\n",
        "\n",
        "For our experiment, as a first step, we shall use 50% of the dataset in an 80-20 train/test split, and we shall discard all users with less than 10 tweets. The following command shows how this can be accomplished (the `--spltseed` flag in this case is to enable reproducible generation of the dataset)\n",
        "\n",
        "After running this script, the `data/sent140/data` directory should contain `train/` and `test/` directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4kwE7zEvSGL",
        "outputId": "3ec04c94-87ff-4e3e-993c-db373cd489c3"
      },
      "source": [
        "os.chdir('../../')\n",
        "os.chdir('data/sent140')\n",
        "\n",
        "! sh ./preprocess.sh --sf 0.5 -t sample -s niid --tf 0.8 -k 3 --spltseed 1549775860"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "retrieving raw data\n",
            "--2021-03-30 10:17:20--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n",
            "--2021-03-30 10:17:20--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  36.9MB/s    in 2.1s    \n",
            "\n",
            "2021-03-30 10:17:23 (36.9 MB/s) - ‘trainingandtestdata.zip’ saved [81363704/81363704]\n",
            "\n",
            "Archive:  trainingandtestdata.zip\n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n",
            "finished retrieving raw data\n",
            "------------------------------\n",
            "combining raw_data .csv files\n",
            "finished combining raw_data .csv files\n",
            "------------------------------\n",
            "converting data to .json format\n",
            "finished converting data to .json format\n",
            "------------------------------\n",
            "sampling data\n",
            "Using seed 1617099525\n",
            "/content/leaf/data/sent140/meta\n",
            "- random seed written out to /content/leaf/data/sent140/meta/sampling_seed.txt\n",
            "writing all_data_niid_5.json\n",
            "------------------------------\n",
            "removing users with less than 3 samples\n",
            "writing all_data_niid_5_keep_3.json\n",
            "------------------------------\n",
            "generating training and test sets\n",
            "- random seed written out to /content/leaf/data/sent140/meta/split_seed.txt\n",
            "splitting data by sample\n",
            "writing all_data_niid_5_keep_3_train_8.json\n",
            "writing all_data_niid_5_keep_3_test_8.json\n",
            "------------------------------\n",
            "calculating JSON file checksums\n",
            "checksums written to meta/dir-checksum.md5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8We64zy1KZ"
      },
      "source": [
        "#### Model Execution\n",
        "\n",
        "Now that we have our data, we can execute our model! For this experiment, the model file is stored at `models/sent140/stacked_lstm.py`. In order train this model using `FedAvg` with 2 clients every round for 10 rounds, we execute the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLYQZjfxy0Sq",
        "outputId": "972da389-c8b3-437d-8e45-f968ef86fbac"
      },
      "source": [
        "os.chdir('../../')\n",
        "os.chdir('models')\n",
        "\n",
        "! python3 ./main.py -dataset sent140 -model stacked_lstm -lr 0.0003 --clients-per-round 2 --num-rounds 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############################## sent140.stacked_lstm ##############################\n",
            "Traceback (most recent call last):\n",
            "  File \"./main.py\", line 186, in <module>\n",
            "    main()\n",
            "  File \"./main.py\", line 58, in main\n",
            "    client_model = ClientModel(args.seed, *model_params)\n",
            "  File \"/content/leaf/models/sent140/stacked_lstm.py\", line 21, in __init__\n",
            "    _, self.indd, vocab = get_word_emb_arr(VOCAB_DIR)\n",
            "  File \"/content/leaf/models/utils/language_utils.py\", line 119, in get_word_emb_arr\n",
            "    with open(path, 'r') as inf:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'sent140/embs.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZRR7NO8qovD"
      },
      "source": [
        "#### Quickstart script\n",
        "\n",
        "This script will execute the instructions provided below for min-sample counts of 3, 10, 30 and 100, reproducibly generating the data partitions and results observed by the authors during analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptL7w_46p4S6",
        "outputId": "0a272b97-e0a0-49f1-b30d-5537c3e0dff8"
      },
      "source": [
        "! sh paper_experiments/sent140.sh paper_experiments"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "leaf/paper_experiments/sent140.sh: 6: leaf/paper_experiments/sent140.sh: Syntax error: \"(\" unexpected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3DZi1RyWDMO"
      },
      "source": [
        ""
      ]
    }
  ]
}